{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import flat_subreddits\n",
    "import os\n",
    "\n",
    "PARENT_PATH = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COMMENTS = 3\n",
    "MAX_COMMENTS = 9999999999\n",
    "\n",
    "MIN_COMMENT_SCORE = 2\n",
    "\n",
    "# Both are inclusive\n",
    "YEAR_START = 2021\n",
    "YEAR_END = 2024\n",
    "\n",
    "MIN_WORDS = 10\n",
    "\n",
    "REMOVAL_PATTERN = r'\\[removed\\]|\\[deleted\\]|Your submission has been removed'\n",
    "\n",
    "posts_schema = {\n",
    "    \"id\": pl.String,\n",
    "    \"title\": pl.String, \n",
    "    \"selftext\": pl.String,\n",
    "    \"author\": pl.String,\n",
    "    \"score\": pl.Int32,\n",
    "    \"num_comments\": pl.Int32,\n",
    "    \"stickied\": pl.Boolean\n",
    "}\n",
    "\n",
    "comments_schema = {\n",
    "    \"id\": pl.String,\n",
    "    \"parent_id\": pl.String,\n",
    "    \"body\": pl.String, \n",
    "    \"score\": pl.Int32,\n",
    "    \"author\": pl.String,\n",
    "    \"stickied\": pl.Boolean,\n",
    "    \"edited\": pl.Int32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_with_utc_handling(filepath, schema):\n",
    "    \"\"\"Read NDJSON file and handle UTC timestamps that may be string or int\"\"\"\n",
    "    \n",
    "    # Read with UTC as string\n",
    "    df_str = pl.read_ndjson(filepath, schema={**schema, \"created_utc\": pl.String})\n",
    "    df_str = df_str.with_columns(\n",
    "        pl.col(\"created_utc\").cast(pl.Int32).alias(\"created_utc_str\")\n",
    "    )\n",
    "    \n",
    "    # Read with UTC as int \n",
    "    df_int = pl.read_ndjson(filepath, schema={\"id\": pl.String, \"created_utc\": pl.Int32})\n",
    "    \n",
    "    # Merge and coalesce UTC fields\n",
    "    return df_str.join(\n",
    "        df_int.select([\"id\", \"created_utc\"])\n",
    "            .rename({\"created_utc\": \"created_utc_int\"}),\n",
    "        on=\"id\",\n",
    "        how=\"left\"\n",
    "    ).with_columns(\n",
    "        pl.coalesce([pl.col(\"created_utc_str\"), pl.col(\"created_utc_int\")]).alias(\"created_utc\")\n",
    "    ).drop([\"created_utc_str\", \"created_utc_int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {}\n",
    "comments = {}\n",
    "\n",
    "for subreddit in flat_subreddits:\n",
    "    posts[subreddit] = read_with_utc_handling(\n",
    "        f\"{PARENT_PATH}/data/extracted/{subreddit}_submissions\",\n",
    "        posts_schema\n",
    "    )\n",
    "\n",
    "    comments[subreddit] = read_with_utc_handling(\n",
    "        f\"{PARENT_PATH}/data/extracted/{subreddit}_comments\", \n",
    "        comments_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_posts(posts):\n",
    "    # Rename columns\n",
    "    posts = posts.rename({\"id\": \"post_id\", \"author\": \"post_author\"})\n",
    "    \n",
    "    # Convert UTC fields to datetime\n",
    "    posts = posts.with_columns(\n",
    "        pl.from_epoch(pl.col(\"created_utc\")).alias(\"created_utc\")\n",
    "    )\n",
    "    \n",
    "    # Specify date frame of interests\n",
    "    posts = posts.filter(\n",
    "        (pl.col(\"created_utc\").dt.year() >= YEAR_START) &\n",
    "        (pl.col(\"created_utc\").dt.year() <= YEAR_END)\n",
    "    )\n",
    "    \n",
    "    # Add temporary text column combining title and selftext\n",
    "    posts = posts.with_columns(\n",
    "        pl.concat_str([\n",
    "            pl.col(\"title\").fill_null(\"\"),\n",
    "            pl.col(\"selftext\").fill_null(\"\")\n",
    "        ], separator=\" \").alias(\"text\")\n",
    "    )\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def prep_comments(comments):\n",
    "    # Rename columns\n",
    "    comments = comments.rename({\"id\": \"comment_id\", \"author\": \"comment_author\"})\n",
    "    \n",
    "    # Only keep first-level comments\n",
    "    comments = comments.with_columns(\n",
    "        pl.col(\"parent_id\").str.replace_all(\"t3_\", \"\").alias(\"post_id\")\n",
    "    ).drop(\"parent_id\")\n",
    "    \n",
    "    # Drop the \"t1_\" prefix to every comment identifier\n",
    "    comments = comments.filter(~pl.col(\"post_id\").str.starts_with(\"t1_\"))\n",
    "    \n",
    "    # Handle edited timestamps\n",
    "    comments = comments.with_columns(\n",
    "        pl.when(pl.col(\"edited\") == 0).then(None).otherwise(pl.col(\"edited\")).alias(\"edited\")\n",
    "    )\n",
    "    \n",
    "    # Convert UTC fields to datetime\n",
    "    comments = comments.with_columns(\n",
    "        pl.from_epoch(pl.col(\"created_utc\")).alias(\"created_utc\"),\n",
    "        pl.from_epoch(pl.col(\"edited\")).alias(\"edited\")\n",
    "    )\n",
    "    \n",
    "    # Convert null \"stickied\" values to false\n",
    "    comments = comments.with_columns(\n",
    "        pl.col(\"stickied\").fill_null(False)\n",
    "    )\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing posts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing posts: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for subreddit in tqdm(flat_subreddits, desc=\"Pre-processing posts\"):\n",
    "    posts[subreddit] = prep_posts(posts[subreddit])\n",
    "    comments[subreddit] = prep_comments(comments[subreddit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering comments:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"post_author\"; valid columns: [\"comment_id\", \"body\", \"score\", \"comment_author\", \"stickied\", \"edited\", \"created_utc\", \"post_id\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'filter' <---\nDF [\"comment_id\", \"body\", \"score\", \"comment_author\"]; PROJECT */8 COLUMNS",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_comments\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subreddit \u001b[38;5;129;01min\u001b[39;00m tqdm(flat_subreddits, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering comments\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     comments[subreddit] \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubreddit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Group comments by post_id and get all comments sorted by score\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     grouped_comments \u001b[38;5;241m=\u001b[39m comments[subreddit]\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mfilter_comments\u001b[1;34m(comments)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_comments\u001b[39m(comments):\n\u001b[0;32m      6\u001b[0m     filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;241m~\u001b[39mpl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstickied\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m      8\u001b[0m         (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m MIN_COMMENT_SCORE) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m         (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcount_matches(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m MIN_WORDS)\n\u001b[0;32m     16\u001b[0m     )\n\u001b[1;32m---> 18\u001b[0m     filtered_comments \u001b[38;5;241m=\u001b[39m \u001b[43mcomments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_comments\n",
      "File \u001b[1;32mc:\\Users\\andrew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\dataframe\\frame.py:5018\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[1;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[0;32m   4864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(\n\u001b[0;32m   4865\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4866\u001b[0m     \u001b[38;5;241m*\u001b[39mpredicates: (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4873\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstraints: Any,\n\u001b[0;32m   4874\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   4875\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4876\u001b[0m \u001b[38;5;124;03m    Filter the rows in the DataFrame based on one or more predicate expressions.\u001b[39;00m\n\u001b[0;32m   4877\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5016\u001b[0m \n\u001b[0;32m   5017\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredicates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andrew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2056\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2056\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mColumnNotFoundError\u001b[0m: unable to find column \"post_author\"; valid columns: [\"comment_id\", \"body\", \"score\", \"comment_author\", \"stickied\", \"edited\", \"created_utc\", \"post_id\"]\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'filter' <---\nDF [\"comment_id\", \"body\", \"score\", \"comment_author\"]; PROJECT */8 COLUMNS"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def filter_comments(comments):\n",
    "\n",
    "    filters = (\n",
    "        ~pl.col(\"stickied\") &\n",
    "        (pl.col(\"score\") >= MIN_COMMENT_SCORE) &\n",
    "        (pl.col(\"comment_author\") != pl.col(\"post_author\")) &\n",
    "        (\n",
    "            pl.col(\"edited\").is_null() |\n",
    "            ((pl.col(\"edited\") - pl.col(\"created_utc\")) <= datetime.timedelta(hours=24))\n",
    "        ) &\n",
    "        ~pl.col(\"body\").str.contains(REMOVAL_PATTERN) &\n",
    "        (pl.col(\"body\").str.count_matches(r'\\S+') >= MIN_WORDS)\n",
    "    )\n",
    "\n",
    "    filtered_comments = comments.filter(filters)\n",
    "    \n",
    "    return filtered_comments\n",
    "\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Filtering comments\"):\n",
    "    comments[subreddit] = filter_comments(comments[subreddit])\n",
    "    # Group comments by post_id and get all comments sorted by score\n",
    "    grouped_comments = comments[subreddit].sort(\"score\", descending=True)\n",
    "\n",
    "    # Get posts with at least 3 comments\n",
    "    comment_counts = grouped_comments.group_by(\"post_id\").agg(\n",
    "        pl.col(\"comment_id\").count().alias(\"n_comments\")\n",
    "    )\n",
    "    valid_posts = comment_counts.filter(pl.col(\"n_comments\") >= MIN_COMMENTS).select(\"post_id\")\n",
    "    filtered_comments = grouped_comments.join(valid_posts, on=\"post_id\")\n",
    "\n",
    "    # For each post, select comments based on count\n",
    "    final_comments = []\n",
    "    post_groups = filtered_comments.partition_by(\"post_id\", as_dict=True)\n",
    "    \n",
    "    for post_comments in post_groups.values():\n",
    "        n_comments = len(post_comments)\n",
    "        \n",
    "        if n_comments < MAX_COMMENTS:\n",
    "            # For posts with 3 or 4 comments, take all\n",
    "            final_comments.append(post_comments)\n",
    "        else:\n",
    "            # For posts with 5 comments\n",
    "            top_2 = post_comments.head(2)\n",
    "            bottom_2 = post_comments.tail(2)\n",
    "            \n",
    "            # Get 1 random middle comment\n",
    "            middle_slice = post_comments.slice(2, n_comments-2)\n",
    "            middle = middle_slice.sample(n=1)\n",
    "            \n",
    "            final_comments.extend([top_2, middle, bottom_2])\n",
    "\n",
    "    # Combine all comments and remove duplicates\n",
    "    final_comments = pl.concat(final_comments).unique(subset=[\"post_id\", \"comment_id\"])\n",
    "\n",
    "    # Update comments and posts with valid posts (3+ comments)\n",
    "    valid_post_ids = final_comments.group_by(\"post_id\").agg(\n",
    "        pl.col(\"comment_id\").count().alias(\"n_comments\")\n",
    "    ).filter(\n",
    "        pl.col(\"n_comments\") >= MIN_COMMENTS\n",
    "    ).select(\"post_id\")\n",
    "\n",
    "    comments[subreddit] = final_comments.join(valid_post_ids, on=\"post_id\")\n",
    "    posts[subreddit] = posts[subreddit].join(valid_post_ids, on=\"post_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making final changes: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def crop_text(text: str, max_words: int = 1000) -> str:\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    return \" \".join(words[:max_words]) + \"...\"\n",
    "\n",
    "def remove_edit_sections(text: str) -> str:\n",
    "    pattern = r\"^(.*?)(?=[\\s]+(?:edit[\\w]{0,5}|update[\\w]{0,5}):)\"\n",
    "    match = re.match(pattern, text, re.IGNORECASE)\n",
    "    result = match.group(1) if match else text\n",
    "    return result\n",
    "\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Making final changes\"):\n",
    "\n",
    "    posts[subreddit] = posts[subreddit].with_columns([\n",
    "        pl.col(\"selftext\").map_elements(crop_text, return_dtype=pl.String).alias(\"selftext\")\n",
    "    ])\n",
    "\n",
    "    comments[subreddit] = comments[subreddit].with_columns([\n",
    "        pl.col(\"body\").map_elements(crop_text, return_dtype=pl.String).alias(\"body\")\n",
    "    ])\n",
    "\n",
    "    posts[subreddit] = posts[subreddit].with_columns([\n",
    "        pl.col(\"selftext\").map_elements(remove_edit_sections, return_dtype=pl.String).alias(\"selftext\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to pandas: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Convert polars dataframes to pandas for snorkel compatibility\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Converting to pandas\"):\n",
    "    posts[subreddit] = posts[subreddit].to_pandas()\n",
    "    comments[subreddit] = comments[subreddit].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit in flat_subreddits:\n",
    "    posts[subreddit].to_csv(f\"{PARENT_PATH}/data/processed/{subreddit}_posts.csv\", index=False)\n",
    "    comments[subreddit].to_csv(f\"{PARENT_PATH}/data/processed/{subreddit}_comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

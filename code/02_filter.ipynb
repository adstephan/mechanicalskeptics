{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PARENT_PATH = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(PARENT_PATH)\n",
    "\n",
    "from utils import flat_subreddits, subreddits\n",
    "\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COMMENTS = 3\n",
    "MAX_COMMENTS = 5\n",
    "\n",
    "MIN_COMMENT_SCORE = 2\n",
    "\n",
    "# Both are inclusive\n",
    "DATE_START = pl.datetime(2020, 11, 30)\n",
    "DATE_END = pl.datetime(2024, 11, 30)\n",
    "\n",
    "MIN_WORDS = 10\n",
    "\n",
    "REMOVAL_PATTERN = r'\\[removed\\]|\\[deleted\\]|Your submission has been removed'\n",
    "\n",
    "posts_schema = {\n",
    "    \"id\": pl.String,\n",
    "    \"title\": pl.String, \n",
    "    \"selftext\": pl.String,\n",
    "    \"author\": pl.String,\n",
    "    \"score\": pl.Int32,\n",
    "    \"num_comments\": pl.Int32,\n",
    "    \"stickied\": pl.Boolean\n",
    "}\n",
    "\n",
    "comments_schema = {\n",
    "    \"id\": pl.String,\n",
    "    \"parent_id\": pl.String,\n",
    "    \"body\": pl.String, \n",
    "    \"score\": pl.Int32,\n",
    "    \"author\": pl.String,\n",
    "    \"stickied\": pl.Boolean,\n",
    "    \"edited\": pl.Int32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_with_utc_handling(filepath, schema):\n",
    "    \"\"\"Read NDJSON file and handle UTC timestamps that may be string or int\"\"\"\n",
    "    \n",
    "    # Read with UTC as string\n",
    "    df_str = pl.read_ndjson(filepath, schema={**schema, \"created_utc\": pl.String})\n",
    "    df_str = df_str.with_columns(\n",
    "        pl.col(\"created_utc\").cast(pl.Int32).alias(\"created_utc_str\")\n",
    "    )\n",
    "    \n",
    "    # Read with UTC as int \n",
    "    df_int = pl.read_ndjson(filepath, schema={\"id\": pl.String, \"created_utc\": pl.Int32})\n",
    "    \n",
    "    # Merge and coalesce UTC fields\n",
    "    return df_str.join(\n",
    "        df_int.select([\"id\", \"created_utc\"])\n",
    "            .rename({\"created_utc\": \"created_utc_int\"}),\n",
    "        on=\"id\",\n",
    "        how=\"left\"\n",
    "    ).with_columns(\n",
    "        pl.coalesce([pl.col(\"created_utc_str\"), pl.col(\"created_utc_int\")]).alias(\"created_utc\")\n",
    "    ).drop([\"created_utc_str\", \"created_utc_int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = {}\n",
    "comments = {}\n",
    "\n",
    "for subreddit in flat_subreddits:\n",
    "    posts[subreddit] = read_with_utc_handling(\n",
    "        f\"{PARENT_PATH}/data/extracted/{subreddit}_submissions\",\n",
    "        posts_schema\n",
    "    )\n",
    "\n",
    "    comments[subreddit] = read_with_utc_handling(\n",
    "        f\"{PARENT_PATH}/data/extracted/{subreddit}_comments\", \n",
    "        comments_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_posts(posts):\n",
    "    # Rename columns\n",
    "    posts = posts.rename({\"id\": \"post_id\", \"author\": \"post_author\"})\n",
    "    \n",
    "    # Convert UTC fields to datetime\n",
    "    posts = posts.with_columns(\n",
    "        pl.from_epoch(pl.col(\"created_utc\")).alias(\"created_utc\")\n",
    "    )\n",
    "    \n",
    "    # Specify date frame of interests\n",
    "    posts = posts.filter(\n",
    "        (pl.col(\"created_utc\") >= DATE_START) &\n",
    "        (pl.col(\"created_utc\") <= DATE_END)\n",
    "    )\n",
    "    \n",
    "    # Add temporary text column combining title and selftext\n",
    "    posts = posts.with_columns(\n",
    "        pl.concat_str([\n",
    "            pl.col(\"title\").fill_null(\"\"),\n",
    "            pl.col(\"selftext\").fill_null(\"\")\n",
    "        ], separator=\" \").alias(\"all_text\")\n",
    "    )\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def prep_comments(comments):\n",
    "    # Rename columns\n",
    "    comments = comments.rename({\"id\": \"comment_id\", \"author\": \"comment_author\"})\n",
    "    \n",
    "    # Only keep first-level comments\n",
    "    comments = comments.with_columns(\n",
    "        pl.col(\"parent_id\").str.replace_all(\"t3_\", \"\").alias(\"post_id\")\n",
    "    ).drop(\"parent_id\")\n",
    "    \n",
    "    # Drop the \"t1_\" prefix to every comment identifier\n",
    "    comments = comments.filter(~pl.col(\"post_id\").str.starts_with(\"t1_\"))\n",
    "    \n",
    "    # Handle edited timestamps\n",
    "    comments = comments.with_columns(\n",
    "        pl.when(pl.col(\"edited\") == 0).then(None).otherwise(pl.col(\"edited\")).alias(\"edited\")\n",
    "    )\n",
    "    \n",
    "    # Convert UTC fields to datetime\n",
    "    comments = comments.with_columns(\n",
    "        pl.from_epoch(pl.col(\"created_utc\")).alias(\"created_utc\"),\n",
    "        pl.from_epoch(pl.col(\"edited\")).alias(\"edited\")\n",
    "    )\n",
    "    \n",
    "    # Convert null \"stickied\" values to false\n",
    "    comments = comments.with_columns(\n",
    "        pl.col(\"stickied\").fill_null(False)\n",
    "    )\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing posts:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-processing posts: 100%|██████████| 5/5 [00:02<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for subreddit in tqdm(flat_subreddits, desc=\"Pre-processing posts\"):\n",
    "    posts[subreddit] = prep_posts(posts[subreddit])\n",
    "    comments[subreddit] = prep_comments(comments[subreddit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering posts: 100%|██████████| 5/5 [00:00<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_posts(posts):\n",
    "    \n",
    "    filters = (\n",
    "        ~pl.col(\"stickied\") &\n",
    "        (pl.col(\"num_comments\") >= MIN_COMMENTS) &\n",
    "        ~(pl.col(\"selftext\").str.contains(REMOVAL_PATTERN) & pl.col(\"title\").str.contains(REMOVAL_PATTERN)) &\n",
    "        (pl.col(\"all_text\").str.count_matches(r'\\S+') >= MIN_WORDS)\n",
    "    )\n",
    "\n",
    "    filtered_posts = posts.filter(filters)\n",
    "\n",
    "    return filtered_posts\n",
    "\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Filtering posts\"):\n",
    "    posts[subreddit] = filter_posts(posts[subreddit])\n",
    "\n",
    "    comments[subreddit] = comments[subreddit].join(\n",
    "        posts[subreddit].select([\"post_id\", \"post_author\"]),\n",
    "        on=\"post_id\",\n",
    "        how=\"inner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering comments: 100%|██████████| 5/5 [00:19<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def filter_comments(comments):\n",
    "\n",
    "    filters = (\n",
    "        ~pl.col(\"stickied\") &\n",
    "        (pl.col(\"score\") >= MIN_COMMENT_SCORE) &\n",
    "        (pl.col(\"comment_author\") != pl.col(\"post_author\")) &\n",
    "        (\n",
    "            pl.col(\"edited\").is_null() |\n",
    "            ((pl.col(\"edited\") - pl.col(\"created_utc\")) <= datetime.timedelta(hours=24))\n",
    "        ) &\n",
    "        ~pl.col(\"body\").str.contains(REMOVAL_PATTERN) &\n",
    "        (pl.col(\"body\").str.count_matches(r'\\S+') >= MIN_WORDS)\n",
    "    )\n",
    "\n",
    "    filtered_comments = comments.filter(filters)\n",
    "    \n",
    "    return filtered_comments\n",
    "\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Filtering comments\"):\n",
    "    comments[subreddit] = filter_comments(comments[subreddit])\n",
    "    # Group comments by post_id and get all comments sorted by score\n",
    "    grouped_comments = comments[subreddit].sort(\"score\", descending=True)\n",
    "\n",
    "    # Get posts with at least 3 comments\n",
    "    comment_counts = grouped_comments.group_by(\"post_id\").agg(\n",
    "        pl.col(\"comment_id\").count().alias(\"n_comments\")\n",
    "    )\n",
    "    valid_posts = comment_counts.filter(pl.col(\"n_comments\") >= MIN_COMMENTS).select(\"post_id\")\n",
    "    filtered_comments = grouped_comments.join(valid_posts, on=\"post_id\")\n",
    "\n",
    "    # For each post, select comments based on count\n",
    "    final_comments = []\n",
    "    post_groups = filtered_comments.partition_by(\"post_id\", as_dict=True)\n",
    "    \n",
    "    for post_comments in post_groups.values():\n",
    "        n_comments = len(post_comments)\n",
    "        \n",
    "        if n_comments < MAX_COMMENTS:\n",
    "            # For posts with 3 or 4 comments, take all\n",
    "            final_comments.append(post_comments)\n",
    "        else:\n",
    "            # For posts with 5 comments\n",
    "            top_2 = post_comments.head(2)\n",
    "            bottom_2 = post_comments.tail(2)\n",
    "            \n",
    "            # Get 1 random middle comment\n",
    "            middle_slice = post_comments.slice(2, n_comments-2)\n",
    "            middle = middle_slice.sample(n=1)\n",
    "            \n",
    "            final_comments.extend([top_2, middle, bottom_2])\n",
    "\n",
    "    # Combine all comments and remove duplicates\n",
    "    final_comments = pl.concat(final_comments).unique(subset=[\"post_id\", \"comment_id\"])\n",
    "\n",
    "    # Update comments and posts with valid posts (3+ comments)\n",
    "    valid_post_ids = final_comments.group_by(\"post_id\").agg(\n",
    "        pl.col(\"comment_id\").count().alias(\"n_comments\")\n",
    "    ).filter(\n",
    "        pl.col(\"n_comments\") >= MIN_COMMENTS\n",
    "    ).select(\"post_id\")\n",
    "\n",
    "    comments[subreddit] = final_comments.join(valid_post_ids, on=\"post_id\")\n",
    "    posts[subreddit] = posts[subreddit].join(valid_post_ids, on=\"post_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making final changes: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def crop_text(text: str, max_words: int = 1000) -> str:\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return text\n",
    "    return \" \".join(words[:max_words]) + \"...\"\n",
    "\n",
    "def remove_edit_sections(text: str) -> str:\n",
    "    pattern = r\"^(.*?)(?=[\\s]+(?:edit[\\w]{0,5}|update[\\w]{0,5}):)\"\n",
    "    match = re.match(pattern, text, re.IGNORECASE)\n",
    "    result = match.group(1) if match else text\n",
    "    return result\n",
    "\n",
    "for subreddit in tqdm(flat_subreddits, desc=\"Making final changes\"):\n",
    "\n",
    "    posts[subreddit] = posts[subreddit].with_columns([\n",
    "        pl.col(\"selftext\").map_elements(crop_text, return_dtype=pl.String).alias(\"selftext\")\n",
    "    ])\n",
    "\n",
    "    comments[subreddit] = comments[subreddit].with_columns([\n",
    "        pl.col(\"body\").map_elements(crop_text, return_dtype=pl.String).alias(\"body\")\n",
    "    ])\n",
    "\n",
    "    posts[subreddit] = posts[subreddit].with_columns([\n",
    "        pl.col(\"selftext\").map_elements(remove_edit_sections, return_dtype=pl.String).alias(\"selftext\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all posts into one dataframe\n",
    "all_posts = pl.concat([\n",
    "    posts[subreddit].with_columns(pl.lit(subreddit).alias(\"subreddit\"))\n",
    "])\n",
    "\n",
    "# Merge all comments into one dataframe \n",
    "all_comments = pl.concat([\n",
    "    comments[subreddit].with_columns(pl.lit(subreddit).alias(\"subreddit\"))\n",
    "    for subreddit in flat_subreddits \n",
    "])\n",
    "\n",
    "# Map subreddits to political leanings\n",
    "subreddit_to_political = {\n",
    "    subreddit: political \n",
    "    for political, subreddit_list in subreddits.items()\n",
    "    for subreddit in subreddit_list\n",
    "}\n",
    "\n",
    "# Add political leaning column\n",
    "all_posts = all_posts.with_columns(\n",
    "    pl.col(\"subreddit\").replace(subreddit_to_political).alias(\"political\")\n",
    ")\n",
    "all_comments = all_comments.with_columns(\n",
    "    pl.col(\"subreddit\").replace(subreddit_to_political).alias(\"political\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stickied and edited columns from posts and comments\n",
    "all_posts = all_posts.drop(['stickied', 'all_text'])\n",
    "all_comments = all_comments.drop(['stickied', 'edited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged dataframes\n",
    "all_posts.write_csv(f\"{PARENT_PATH}/data/processed/all_posts.csv\")\n",
    "all_comments.write_csv(f\"{PARENT_PATH}/data/processed/all_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert polars dataframes to pandas for snorkel compatibility\n",
    "# for subreddit in tqdm(flat_subreddits, desc=\"Converting to pandas\"):\n",
    "#     posts[subreddit] = posts[subreddit].to_pandas()\n",
    "#     comments[subreddit] = comments[subreddit].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subreddit in flat_subreddits:\n",
    "#     posts[subreddit].to_csv(f\"{PARENT_PATH}/data/processed/{subreddit}_posts.csv\", index=False)\n",
    "#     comments[subreddit].to_csv(f\"{PARENT_PATH}/data/processed/{subreddit}_comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
